#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é˜…è¯»é‡ã€ç‚¹èµé‡
é€‚ç”¨äºå¤§é‡æ–‡ç« åœºæ™¯
"""

import requests
import json
import time
import re
import os
from urllib.parse import urlparse, parse_qs
from datetime import datetime
import argparse

class BatchStatsScraper:
    def __init__(self, config_file="wechat_config.json"):
        self.session = requests.Session()
        self.config = self.load_config(config_file)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.setup_session()
    
    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨")
            print("è¯·å…ˆè¿è¡Œ wechat_client_scraper.py åˆ›å»ºé…ç½®æ¨¡æ¿")
            return None
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        if not self.config:
            return
        
        # è®¾ç½®cookies
        cookies = self.config.get('cookies', {})
        self.session.cookies.update(cookies)
        
        # è®¾ç½®appmsg_token
        self.appmsg_token = cookies.get('appmsg_token', '')
        
        print(f"âœ… ä¼šè¯è®¾ç½®å®Œæˆ")
        print(f"   Token: {self.appmsg_token[:20]}...")
    
    def expand_short_url(self, url):
        """å±•å¼€çŸ­é“¾æ¥ä¸ºå®Œæ•´é“¾æ¥"""
        try:
            if '/s/' in url:  # çŸ­é“¾æ¥æ ¼å¼
                print(f"ğŸ”— æ£€æµ‹åˆ°çŸ­é“¾æ¥ï¼Œæ­£åœ¨å±•å¼€...")
                # ç¦ç”¨SSLéªŒè¯
                response = self.session.get(url, headers=self.headers, timeout=10, allow_redirects=False, verify=False)
                if response.status_code in [301, 302]:
                    expanded_url = response.headers.get('Location')
                    print(f"   âœ… å±•å¼€æˆåŠŸ: {expanded_url}")
                    return expanded_url
                else:
                    print(f"   âš ï¸  æ— æ³•å±•å¼€çŸ­é“¾æ¥ï¼Œå°è¯•ç›´æ¥è®¿é—®...")
                    # å°è¯•ç›´æ¥è®¿é—®è·å–å®Œæ•´URL
                    response = self.session.get(url, headers=self.headers, timeout=10, allow_redirects=True, verify=False)
                    final_url = response.url
                    if final_url != url:
                        print(f"   âœ… è·å–åˆ°å®Œæ•´URL: {final_url}")
                        return final_url
                    else:
                        print(f"   âŒ çŸ­é“¾æ¥å±•å¼€å¤±è´¥ï¼Œæ— æ³•è·å–å‚æ•°")
                        return None
            return url
        except Exception as e:
            print(f"   âš ï¸  å±•å¼€çŸ­é“¾æ¥å¤±è´¥: {e}")
            return None

    def extract_params_from_url(self, url):
        """ä»URLä¸­æå–å¿…è¦å‚æ•°"""
        try:
            # å…ˆå±•å¼€çŸ­é“¾æ¥
            full_url = self.expand_short_url(url)
            
            if not full_url:
                print(f"âŒ æ— æ³•è·å–å®Œæ•´URLï¼Œè·³è¿‡æ­¤æ–‡ç« ")
                return None
            
            parsed = urlparse(full_url)
            query_params = parse_qs(parsed.query)
            
            required_params = ['__biz', 'mid', 'idx', 'sn']
            params = {}
            
            for param in required_params:
                value = query_params.get(param, [''])[0]
                if not value:
                    print(f"âŒ URLç¼ºå°‘å¿…è¦å‚æ•°: {param}")
                    print(f"   å½“å‰URLå‚æ•°: {list(query_params.keys())}")
                    print(f"   å®Œæ•´URL: {full_url}")
                    return None
                params[param] = value
            
            return params
        except Exception as e:
            print(f"âŒ URLå‚æ•°æå–å¤±è´¥: {e}")
            return None
    
    def get_article_stats(self, url, retry_count=3):
        """è·å–å•ç¯‡æ–‡ç« ç»Ÿè®¡æ•°æ®"""
        for attempt in range(retry_count):
            try:
                # æå–URLå‚æ•°
                params = self.extract_params_from_url(url)
                if not params:
                    return None
                
                # æ„å»ºAPIè¯·æ±‚
                api_url = "https://mp.weixin.qq.com/mp/getappmsgext"
                data = {
                    '__biz': params['__biz'],
                    'mid': params['mid'],
                    'sn': params['sn'],
                    'idx': params['idx'],
                    'appmsg_type': '9',
                    'appmsg_token': self.appmsg_token,
                    'f': 'json',
                    'pass_ticket': '',
                    'wxtoken': '',
                    'x5': '0'
                }
                
                # å‘é€è¯·æ±‚
                response = self.session.post(api_url, data=data, headers=self.headers, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                    if 'appmsgstat' in result:
                        stats = result['appmsgstat']
                        return {
                            'read_num': stats.get('read_num', 0),
                            'like_num': stats.get('like_num', 0),
                            'reward_num': stats.get('reward_num', 0),
                            'reward_total_count': stats.get('reward_total_count', 0),
                            'success': True
                        }
                    else:
                        print(f"âš ï¸  APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {result}")
                        return {'success': False, 'error': 'æ•°æ®æ ¼å¼å¼‚å¸¸'}
                else:
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    if attempt < retry_count - 1:
                        time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue
                    return {'success': False, 'error': f'HTTP {response.status_code}'}
                    
            except requests.exceptions.Timeout:
                print(f"â° è¯·æ±‚è¶…æ—¶ï¼Œå°è¯•é‡è¯•...")
                if attempt < retry_count - 1:
                    time.sleep(2 ** attempt)
                    continue
                return {'success': False, 'error': 'è¯·æ±‚è¶…æ—¶'}
            except Exception as e:
                print(f"âŒ è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
                if attempt < retry_count - 1:
                    time.sleep(2 ** attempt)
                    continue
                return {'success': False, 'error': str(e)}
        
        return {'success': False, 'error': 'é‡è¯•æ¬¡æ•°å·²ç”¨å®Œ'}
    
    def batch_get_stats(self, urls, delay=1.0):
        """æ‰¹é‡è·å–ç»Ÿè®¡æ•°æ®"""
        if not self.config:
            print("âŒ è¯·å…ˆé…ç½®å¾®ä¿¡å‚æ•°")
            return None
        
        results = []
        total = len(urls)
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å– {total} ç¯‡æ–‡ç« çš„ç»Ÿè®¡æ•°æ®")
        print(f"   å»¶è¿Ÿè®¾ç½®: {delay} ç§’")
        print(f"   é¢„è®¡æ—¶é—´: {total * delay / 60:.1f} åˆ†é’Ÿ")
        print()
        
        for i, url in enumerate(urls, 1):
            print(f"[{i}/{total}] æ­£åœ¨å¤„ç†: {url}")
            
            # è·å–ç»Ÿè®¡æ•°æ®
            stats = self.get_article_stats(url)
            
            if stats and stats.get('success'):
                results.append({
                    'url': url,
                    'read_count': stats['read_num'],
                    'like_count': stats['like_num'],
                    'reward_count': stats['reward_num'],
                    'reward_total_count': stats['reward_total_count'],
                    'success': True,
                    'timestamp': datetime.now().isoformat()
                })
                print(f"   âœ… æˆåŠŸ: é˜…è¯»é‡={stats['read_num']}, ç‚¹èµé‡={stats['like_num']}")
            else:
                error_msg = stats.get('error', 'æœªçŸ¥é”™è¯¯') if stats else 'è¯·æ±‚å¤±è´¥'
                results.append({
                    'url': url,
                    'success': False,
                    'error': error_msg,
                    'timestamp': datetime.now().isoformat()
                })
                print(f"   âŒ å¤±è´¥: {error_msg}")
            
            # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < total:
                time.sleep(delay)
        
        return results
    
    def save_results(self, results, output_file="stats_results.json"):
        """ä¿å­˜ç»“æœ"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if r.get('success'))
            fail_count = len(results) - success_count
            
            print(f"\nğŸ‰ æ‰¹é‡è·å–å®Œæˆï¼")
            print(f"   æ€»è®¡: {len(results)} ç¯‡")
            print(f"   æˆåŠŸ: {success_count} ç¯‡")
            print(f"   å¤±è´¥: {fail_count} ç¯‡")
            print(f"   æˆåŠŸç‡: {success_count/len(results)*100:.1f}%")
            print(f"   ç»“æœå·²ä¿å­˜: {output_file}")
            
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False

def load_article_urls(input_file):
    """åŠ è½½æ–‡ç« URLåˆ—è¡¨"""
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        urls = []
        for item in data:
            if isinstance(item, dict):
                url = item.get('url') or item.get('link')
                if url:
                    urls.append(url)
            elif isinstance(item, str):
                urls.append(item)
        
        return urls
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç»Ÿè®¡æ•°æ®')
    parser.add_argument('--input', '-i', default='articles_detailed.json', 
                       help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: articles_detailed.json)')
    parser.add_argument('--output', '-o', default='stats_results.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: stats_results.json)')
    parser.add_argument('--config', '-c', default='wechat_config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: wechat_config.json)')
    parser.add_argument('--delay', '-d', type=float, default=1.0,
                       help='è¯·æ±‚å»¶è¿Ÿç§’æ•° (é»˜è®¤: 1.0)')
    parser.add_argument('--urls', nargs='+',
                       help='ç›´æ¥æŒ‡å®šURLåˆ—è¡¨')
    
    args = parser.parse_args()
    
    print("ğŸ“Š æ‰¹é‡è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç»Ÿè®¡æ•°æ®")
    print("="*50)
    
    # åˆå§‹åŒ–æŠ“å–å™¨
    scraper = BatchStatsScraper(args.config)
    
    # è·å–URLåˆ—è¡¨
    if args.urls:
        urls = args.urls
        print(f"ğŸ“ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ {len(urls)} ä¸ªURL")
    else:
        urls = load_article_urls(args.input)
        if not urls:
            print(f"âŒ æ— æ³•ä» {args.input} åŠ è½½URLåˆ—è¡¨")
            return
        print(f"ğŸ“ ä» {args.input} åŠ è½½äº† {len(urls)} ä¸ªURL")
    
    # ç¡®è®¤å¼€å§‹
    print(f"\nâš ï¸  å³å°†å¼€å§‹æ‰¹é‡è·å– {len(urls)} ç¯‡æ–‡ç« çš„ç»Ÿè®¡æ•°æ®")
    print(f"   é¢„è®¡æ—¶é—´: {len(urls) * args.delay / 60:.1f} åˆ†é’Ÿ")
    confirm = input("æ˜¯å¦ç»§ç»­? (y/n): ").strip().lower()
    
    if confirm not in ['y', 'yes', 'æ˜¯']:
        print("ğŸ‘‹ å·²å–æ¶ˆæ“ä½œ")
        return
    
    # å¼€å§‹æ‰¹é‡è·å–
    results = scraper.batch_get_stats(urls, args.delay)
    
    if results:
        scraper.save_results(results, args.output)

if __name__ == "__main__":
    main() 