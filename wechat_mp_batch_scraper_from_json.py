import json
import time
import re
import os
import requests
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright
from urllib.parse import urlparse, unquote
from pathlib import Path

def parse_date(date_str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not date_str:
        return None
    
    date_formats = [
        '%Y-%m-%d',
        '%Yå¹´%mæœˆ%dæ—¥',
        '%Y/%m/%d',
        '%Y.%m.%d'
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    
    return None

def filter_articles_by_date(articles, start_date=None, end_date=None):
    """æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ç« """
    filtered_articles = []
    
    for article in articles:
        article_date_str = article.get('date', '')
        article_date = parse_date(article_date_str)
        
        if not article_date:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
        if start_date and article_date < start_date:
            continue
        if end_date and article_date > end_date:
            continue
        
        filtered_articles.append(article)
    
    return filtered_articles

def get_preset_date_range(option):
    """è·å–é¢„è®¾çš„æ—¶é—´èŒƒå›´"""
    today = datetime.now()
    
    if option == "1":  # è¿‡å»ä¸€å¹´
        start_date = today - timedelta(days=365)
        return start_date, today, "è¿‡å»ä¸€å¹´"
    elif option == "2":  # è¿‡å»åŠå¹´
        start_date = today - timedelta(days=180)
        return start_date, today, "è¿‡å»åŠå¹´"
    elif option == "3":  # è¿‡å»ä¸‰ä¸ªæœˆ
        start_date = today - timedelta(days=90)
        return start_date, today, "è¿‡å»ä¸‰ä¸ªæœˆ"
    elif option == "4":  # è¿‡å»ä¸€ä¸ªæœˆ
        start_date = today - timedelta(days=30)
        return start_date, today, "è¿‡å»ä¸€ä¸ªæœˆ"
    elif option == "5":  # è¿‡å»ä¸€å‘¨
        start_date = today - timedelta(days=7)
        return start_date, today, "è¿‡å»ä¸€å‘¨"
    elif option == "6":  # ä»Šå¹´
        start_date = datetime(today.year, 1, 1)
        return start_date, today, "ä»Šå¹´"
    elif option == "7":  # å»å¹´
        start_date = datetime(today.year - 1, 1, 1)
        end_date = datetime(today.year - 1, 12, 31)
        return start_date, end_date, "å»å¹´"
    else:
        return None, None, None

def get_custom_date_range():
    """è·å–ç”¨æˆ·è‡ªå®šä¹‰çš„æ—¶é—´èŒƒå›´"""
    print("\nè¯·è¾“å…¥è‡ªå®šä¹‰æ—¶é—´èŒƒå›´ï¼š")
    
    while True:
        start_str = input("å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2024-01-01): ").strip()
        if not start_str:
            return None, None
        
        start_date = parse_date(start_str)
        if start_date:
            break
        else:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    while True:
        end_str = input("ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2024-12-31): ").strip()
        if not end_str:
            end_date = datetime.now()
            break
        
        end_date = parse_date(end_str)
        if end_date:
            break
        else:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    return start_date, end_date

def show_time_range_menu():
    """æ˜¾ç¤ºæ—¶é—´èŒƒå›´é€‰æ‹©èœå•"""
    print("\n" + "="*50)
    print("ğŸ“… è¯·é€‰æ‹©æŠ“å–æ–‡ç« çš„æ—¶é—´èŒƒå›´ï¼š")
    print("="*50)
    print("1. è¿‡å»ä¸€å¹´ (365å¤©)")
    print("2. è¿‡å»åŠå¹´ (180å¤©)")
    print("3. è¿‡å»ä¸‰ä¸ªæœˆ (90å¤©)")
    print("4. è¿‡å»ä¸€ä¸ªæœˆ (30å¤©)")
    print("5. è¿‡å»ä¸€å‘¨ (7å¤©)")
    print("6. ä»Šå¹´ (1æœˆ1æ—¥è‡³ä»Š)")
    print("7. å»å¹´ (å…¨å¹´)")
    print("8. è‡ªå®šä¹‰æ—¶é—´èŒƒå›´")
    print("9. æŠ“å–æ‰€æœ‰æ–‡ç« ")
    print("0. é€€å‡ºç¨‹åº")
    print("="*50)

def download_image(img_url: str, save_dir: str) -> str:
    """
    ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„
    """
    try:
        # è§£æURLï¼Œè·å–æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        img_name = os.path.basename(unquote(parsed_url.path))
        
        # ç¡®ä¿æ–‡ä»¶åæœ‰æ•ˆä¸”å”¯ä¸€
        img_name = re.sub(r'[<>:"/\\|?*]', '_', img_name)  # æ›¿æ¢éæ³•å­—ç¬¦
        if not img_name or len(img_name) > 255:  # å¤„ç†æ— æ•ˆæˆ–è¿‡é•¿çš„æ–‡ä»¶å
            img_name = f"img_{int(time.time()*1000)}.jpg"
        
        # æ„å»ºä¿å­˜è·¯å¾„
        img_path = os.path.join(save_dir, img_name)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ•°å­—åç¼€
        base, ext = os.path.splitext(img_path)
        counter = 1
        while os.path.exists(img_path):
            img_path = f"{base}_{counter}{ext}"
            counter += 1
        
        # ä¸‹è½½å›¾ç‰‡
        response = requests.get(img_url, timeout=30)
        response.raise_for_status()
        
        # ä¿å­˜å›¾ç‰‡
        with open(img_path, 'wb') as f:
            f.write(response.content)
        
        return img_path
    except Exception as e:
        print(f"    âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ ({img_url}): {e}")
        return None

def html_to_markdown(element, page, images_dir) -> str:
    """
    å°†HTMLå…ƒç´ è½¬æ¢ä¸ºMarkdownæ ¼å¼
    """
    if not element:
        return ""
    
    markdown_content = []
    
    # è·å–æ‰€æœ‰å­å…ƒç´ 
    children = element.query_selector_all('*')
    
    for child in children:
        # è·å–å…ƒç´ æ ‡ç­¾åå’Œç±»å
        tag_name = child.evaluate('element => element.tagName.toLowerCase()')
        class_name = child.get_attribute('class') or ''
        
        # è·å–å…ƒç´ æ–‡æœ¬
        text = child.inner_text().strip()
        if not text:
            continue
        
        # å¤„ç†æ ‡é¢˜
        if tag_name == 'h1':
            markdown_content.append(f"\n# {text}\n")
        elif tag_name == 'h2':
            markdown_content.append(f"\n## {text}\n")
        elif tag_name == 'h3':
            markdown_content.append(f"\n### {text}\n")
        elif tag_name == 'h4':
            markdown_content.append(f"\n#### {text}\n")
        
        # å¤„ç†æ®µè½
        elif tag_name == 'p':
            # æ£€æŸ¥æ˜¯å¦ä¸ºåŠ ç²—æ–‡æœ¬
            if 'strong' in class_name.lower() or child.query_selector('strong'):
                markdown_content.append(f"\n**{text}**\n")
            else:
                markdown_content.append(f"\n{text}\n")
        
        # å¤„ç†åˆ—è¡¨
        elif tag_name in ['ul', 'ol']:
            items = child.query_selector_all('li')
            for idx, item in enumerate(items):
                if tag_name == 'ul':
                    markdown_content.append(f"- {item.inner_text().strip()}\n")
                else:
                    markdown_content.append(f"{idx+1}. {item.inner_text().strip()}\n")
        
        # å¤„ç†å›¾ç‰‡
        elif tag_name == 'img':
            img_url = child.get_attribute('data-src') or child.get_attribute('src')
            if img_url:
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                
                # ä¸‹è½½å›¾ç‰‡
                local_path = download_image(img_url, images_dir)
                if local_path:
                    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                    rel_path = os.path.relpath(local_path, os.path.dirname(images_dir))
                    markdown_content.append(f"\n![å›¾ç‰‡]({rel_path})\n")
        
        # å¤„ç†å¼•ç”¨
        elif 'blockquote' in class_name.lower():
            markdown_content.append(f"\n> {text}\n")
        
        # å¤„ç†ä»£ç å—
        elif 'code' in class_name.lower():
            markdown_content.append(f"\n```\n{text}\n```\n")
    
    return '\n'.join(markdown_content)

def fetch_article_content(url, folder_name, retry_count=5):
    """æŠ“å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ”¯æŒé‡è¯•"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        page.set_default_timeout(45000)
        
        for attempt in range(retry_count):
            try:
                print(f"    å°è¯•ç¬¬ {attempt + 1} æ¬¡è®¿é—®...")
                page.goto(url, timeout=45000)
                
                # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
                page.wait_for_load_state('domcontentloaded', timeout=20000)
                
                # é¢å¤–ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿å†…å®¹åŠ è½½
                page.wait_for_timeout(5000)
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ•ˆï¼ˆæ–‡ç« æ˜¯å¦å­˜åœ¨ï¼‰
                page_content = page.content()
                
                # æ£€æµ‹åˆ é™¤/è¿è§„é¡µé¢çš„å…³é”®è¯
                invalid_keywords = [
                    "æ­¤å†…å®¹å‘é€å¤±è´¥æ— æ³•æŸ¥çœ‹",
                    "æ­¤å†…å®¹å› æ¶‰å«Œè¿åç›¸å…³æ³•å¾‹æ³•è§„å’Œæ”¿ç­–å‘é€å¤±è´¥",
                    "å†…å®¹å·²åˆ é™¤",
                    "æ–‡ç« ä¸å­˜åœ¨",
                    "è¯¥å†…å®¹å·²è¢«åˆ é™¤",
                    "å†…å®¹è¿è§„",
                    "æ— æ³•æŸ¥çœ‹",
                    "å‘é€å¤±è´¥",
                    "è¿è§„å†…å®¹",
                    "å†…å®¹ä¸å­˜åœ¨"
                ]
                
                is_invalid_page = any(keyword in page_content for keyword in invalid_keywords)
                
                if is_invalid_page:
                    print(f"    âš ï¸  æ£€æµ‹åˆ°æ— æ•ˆé¡µé¢ï¼Œæ–‡ç« å¯èƒ½å·²è¢«åˆ é™¤æˆ–è¿è§„")
                    browser.close()
                    return {
                        'url': url,
                        'title': '',
                        'author': '',
                        'publish_time': '',
                        'read_count': '',
                        'like_count': '',
                        'content': '',
                        'error': 'æ–‡ç« å·²è¢«åˆ é™¤æˆ–è¿è§„ï¼Œæ— æ³•æŸ¥çœ‹',
                        'status': 'deleted'
                    }
                
                # æå–æ–‡ç« ä¿¡æ¯
                article_data = {
                    'url': url,
                    'title': '',
                    'author': '',
                    'publish_time': '',
                    'read_count': '',
                    'like_count': '',
                    'content': ''
                }
                
                # æå–æ ‡é¢˜
                try:
                    title_element = page.query_selector('h1.rich_media_title')
                    if title_element:
                        article_data['title'] = title_element.inner_text().strip()
                except:
                    pass
                
                # æå–ä½œè€…
                try:
                    author_element = page.query_selector('a#js_name')
                    if author_element:
                        article_data['author'] = author_element.inner_text().strip()
                except:
                    pass
                
                # æå–å‘å¸ƒæ—¶é—´
                try:
                    time_element = page.query_selector('em#publish_time')
                    if time_element:
                        article_data['publish_time'] = time_element.inner_text().strip()
                except:
                    pass
                
                # æå–é˜…è¯»é‡
                try:
                    read_element = page.query_selector('span#js_read_area')
                    if read_element:
                        read_text = read_element.inner_text().strip()
                        # æå–æ•°å­—
                        read_match = re.search(r'(\d+)', read_text)
                        if read_match:
                            article_data['read_count'] = read_match.group(1)
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        read_elements = page.query_selector_all('span')
                        for element in read_elements:
                            text = element.inner_text().strip()
                            if 'é˜…è¯»' in text and re.search(r'\d+', text):
                                read_match = re.search(r'(\d+)', text)
                                if read_match:
                                    article_data['read_count'] = read_match.group(1)
                                    break
                except:
                    pass
                
                # æå–ç‚¹èµé‡
                try:
                    like_element = page.query_selector('span#like_area')
                    if like_element:
                        like_text = like_element.inner_text().strip()
                        like_match = re.search(r'(\d+)', like_text)
                        if like_match:
                            article_data['like_count'] = like_match.group(1)
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        like_elements = page.query_selector_all('span')
                        for element in like_elements:
                            text = element.inner_text().strip()
                            if 'èµ' in text and re.search(r'\d+', text):
                                like_match = re.search(r'(\d+)', text)
                                if like_match:
                                    article_data['like_count'] = like_match.group(1)
                                    break
                except:
                    pass
                
                # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
                images_dir = os.path.join(folder_name, 'images')
                os.makedirs(images_dir, exist_ok=True)
                
                # æå–æ­£æ–‡å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
                try:
                    content_element = page.query_selector('div#js_content')
                    if content_element:
                        # å°†å†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼
                        article_data['content'] = html_to_markdown(content_element, page, images_dir)
                except Exception as e:
                    print(f"    âš ï¸ å†…å®¹è½¬æ¢å¤±è´¥: {e}")
                    pass
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸæŠ“å–åˆ°æœ‰æ•ˆå†…å®¹
                if article_data.get('title') and len(article_data.get('title', '').strip()) > 0:
                    browser.close()
                    return article_data
                else:
                    # å¦‚æœæ²¡æœ‰æŠ“å–åˆ°æ ‡é¢˜ï¼Œç»§ç»­é‡è¯•
                    raise Exception("æœªæŠ“å–åˆ°æ–‡ç« æ ‡é¢˜ï¼Œå¯èƒ½é¡µé¢æœªå®Œå…¨åŠ è½½")
                
            except Exception as e:
                print(f"    ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < retry_count - 1:
                    # é€’å¢ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    wait_time = (attempt + 1) * 3
                    print(f"    ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"    æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    browser.close()
                    return {
                        'url': url,
                        'title': '',
                        'author': '',
                        'publish_time': '',
                        'read_count': '',
                        'like_count': '',
                        'content': '',
                        'error': f"é‡è¯• {retry_count} æ¬¡åä»ç„¶å¤±è´¥: {str(e)}"
                    }

def main():
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡æŠ“å–å·¥å…· (JSON ç‰ˆ)")
    print("ä» ArticleList.json æ–‡ä»¶è¯»å–æ–‡ç« é“¾æ¥å¹¶æ‰¹é‡æŠ“å–")
    
    # è¯»å– ArticleList.json æ–‡ä»¶
    try:
        with open("ArticleList.json", "r", encoding="utf-8") as f:
            all_articles = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å– ArticleList.jsonï¼Œå…± {len(all_articles)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"âŒ è¯»å– ArticleList.json å¤±è´¥: {e}")
        return
    
    # æ˜¾ç¤ºæ—¶é—´èŒƒå›´é€‰æ‹©èœå•
    while True:
        show_time_range_menu()
        choice = input("è¯·é€‰æ‹© (0-9): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")
            return
        
        elif choice == "9":
            # æŠ“å–æ‰€æœ‰æ–‡ç« 
            start_date = None
            end_date = None
            range_name = "æ‰€æœ‰æ–‡ç« "
            break
        
        elif choice == "8":
            # è‡ªå®šä¹‰æ—¶é—´èŒƒå›´
            start_date, end_date = get_custom_date_range()
            if start_date and end_date:
                range_name = f"è‡ªå®šä¹‰èŒƒå›´ ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})"
                break
            else:
                print("âŒ è‡ªå®šä¹‰æ—¶é—´èŒƒå›´è®¾ç½®å¤±è´¥")
                continue
        
        elif choice in ["1", "2", "3", "4", "5", "6", "7"]:
            # é¢„è®¾æ—¶é—´èŒƒå›´
            start_date, end_date, range_name = get_preset_date_range(choice)
            if start_date and end_date:
                break
            else:
                print("âŒ é¢„è®¾æ—¶é—´èŒƒå›´è·å–å¤±è´¥")
                continue
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
    
    # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ç« 
    if start_date or end_date:
        filtered_articles = filter_articles_by_date(all_articles, start_date, end_date)
        print(f"\nğŸ“Š æ—¶é—´èŒƒå›´: {range_name}")
        print(f"ğŸ“… ç­›é€‰æ¡ä»¶: {start_date.strftime('%Y-%m-%d') if start_date else 'ä¸é™'} è‡³ {end_date.strftime('%Y-%m-%d') if end_date else 'ä¸é™'}")
        print(f"ğŸ“ ç­›é€‰ç»“æœ: {len(filtered_articles)} ç¯‡æ–‡ç« ")
        
        if len(filtered_articles) == 0:
            print("âŒ è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
            return
        
        # ç¡®è®¤æ˜¯å¦ç»§ç»­
        confirm = input(f"\næ˜¯å¦å¼€å§‹æŠ“å–è¿™ {len(filtered_articles)} ç¯‡æ–‡ç« ï¼Ÿ(y/n): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("ğŸ‘‹ å·²å–æ¶ˆæŠ“å–")
            return
    else:
        filtered_articles = all_articles
        print(f"\nğŸ“Š æ—¶é—´èŒƒå›´: {range_name}")
        print(f"ğŸ“ å°†æŠ“å–æ‰€æœ‰ {len(filtered_articles)} ç¯‡æ–‡ç« ")
    
    # æå–æ‰€æœ‰é“¾æ¥
    urls = []
    for item in filtered_articles:
        if 'link' in item:
            urls.append(item['link'])
        elif 'url' in item:
            urls.append(item['url'])
    
    print(f"\nğŸš€ å¼€å§‹æŠ“å– {len(urls)} ç¯‡æ–‡ç« ...")
    
    # æ‰¹é‡æŠ“å–æ–‡ç« 
    articles = []
    for idx, url in enumerate(urls, 1):
        print(f"[{idx}/{len(urls)}] æ­£åœ¨æŠ“å–: {url}")
        
        try:
            # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
            images_dir = os.path.join(os.path.dirname(__file__), "images") # å‡è®¾è„šæœ¬åœ¨å½“å‰ç›®å½•ä¸‹
            os.makedirs(images_dir, exist_ok=True)

            article_data = fetch_article_content(url, images_dir)
            articles.append(article_data)
            
            # æ˜¾ç¤ºæŠ“å–ç»“æœ
            if article_data.get('title'):
                print(f"  âœ… æˆåŠŸ: {article_data['title'][:50]}...")
            else:
                print(f"  âŒ å¤±è´¥: æœªè·å–åˆ°æ ‡é¢˜")
            
        except Exception as e:
            print(f"  âŒ æŠ“å–å¼‚å¸¸: {e}")
            articles.append({
                'url': url,
                'title': '',
                'author': '',
                'publish_time': '',
                'read_count': '',
                'like_count': '',
                'content': '',
                'error': str(e)
            })
        
        # é˜²æ­¢è¿‡å¿«è¢«å°ï¼Œæ¯æ¬¡æŠ“å–é—´éš” 5 ç§’
        time.sleep(5)
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"articles_batch_{timestamp}"
    
    # ç¡®ä¿æ–‡ä»¶å¤¹åç§°å”¯ä¸€
    counter = 1
    original_folder_name = folder_name
    while os.path.exists(folder_name):
        folder_name = f"{original_folder_name}_{counter}"
        counter += 1
    
    # åˆ›å»ºæ–‡ä»¶å¤¹
    os.makedirs(folder_name, exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {folder_name}")
    
    # ä¿å­˜ç»“æœæ–‡ä»¶
    output_file = os.path.join(folder_name, "articles_detailed.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for article in articles if article.get('title'))
    fail_count = len(articles) - success_count
    success_rate = success_count/len(articles)*100 if len(articles) > 0 else 0
    
    # åˆ†æå¤±è´¥åŸå› 
    failed_articles = [article for article in articles if not article.get('title')]
    deleted_articles = [article for article in articles if article.get('status') == 'deleted']
    error_analysis = {}
    
    for article in failed_articles:
        error_msg = article.get('error', 'æœªçŸ¥é”™è¯¯')
        error_analysis[error_msg] = error_analysis.get(error_msg, 0) + 1
    
    # ä¿å­˜æŠ“å–ä¿¡æ¯
    info_file = os.path.join(folder_name, "crawl_info.json")
    crawl_info = {
        "crawl_time": datetime.now().isoformat(),
        "time_range": range_name,
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "total_articles": len(articles),
        "success_count": success_count,
        "fail_count": fail_count,
        "deleted_count": len(deleted_articles),
        "success_rate": f"{success_rate:.1f}%",
        "source_file": "ArticleList.json",
        "error_analysis": error_analysis
    }
    with open(info_file, "w", encoding="utf-8") as f:
        json.dump(crawl_info, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ—¶é—´èŒƒå›´: {range_name}")
    print(f"   æ€»è®¡: {len(articles)} ç¯‡æ–‡ç« ")
    print(f"   æˆåŠŸ: {success_count} ç¯‡")
    print(f"   å¤±è´¥: {fail_count} ç¯‡")
    if len(deleted_articles) > 0:
        print(f"   ğŸ“„ å…¶ä¸­å·²åˆ é™¤/è¿è§„: {len(deleted_articles)} ç¯‡")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
    
    # æ˜¾ç¤ºå¤±è´¥åˆ†æ
    if fail_count > 0:
        print(f"\nâŒ å¤±è´¥åˆ†æ:")
        for error_msg, count in error_analysis.items():
            print(f"   {error_msg}: {count} ç¯‡")
        
        # æä¾›æ”¹è¿›å»ºè®®
        if success_rate < 80 and len(deleted_articles) == 0:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            print(f"   - æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æŠ“å–é—´éš”æ—¶é—´")
            print(f"   - å¯ä»¥å°è¯•åœ¨ç½‘ç»œè¾ƒå¥½çš„æ—¶æ®µè¿›è¡ŒæŠ“å–")
            print(f"   - è€ƒè™‘åˆ†æ‰¹æŠ“å–ï¼Œå‡å°‘å•æ¬¡æŠ“å–æ•°é‡")
        elif len(deleted_articles) > 0:
            print(f"\nğŸ’¡ è¯´æ˜:")
            print(f"   - éƒ¨åˆ†æ–‡ç« å·²è¢«åˆ é™¤æˆ–è¿è§„ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡")
            print(f"   - ç¨‹åºå·²è‡ªåŠ¨è¯†åˆ«å¹¶è·³è¿‡è¿™äº›æ— æ•ˆé¡µé¢ï¼ŒèŠ‚çœäº†æŠ“å–æ—¶é—´")
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {folder_name}")
    print(f"   ğŸ“„ æ–‡ç« æ•°æ®: {output_file}")
    print(f"   ğŸ“‹ æŠ“å–ä¿¡æ¯: {info_file}")

if __name__ == "__main__":
    main() 