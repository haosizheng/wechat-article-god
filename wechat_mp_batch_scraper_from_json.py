import json
import time
import re
import os
import requests
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright
from urllib.parse import urlparse, unquote
from pathlib import Path

def parse_date(date_str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not date_str:
        return None
    
    date_formats = [
        '%Y-%m-%d',
        '%Yå¹´%mæœˆ%dæ—¥',
        '%Y/%m/%d',
        '%Y.%m.%d'
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    
    return None

def filter_articles_by_date(articles, start_date=None, end_date=None):
    """æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ç« """
    filtered_articles = []
    
    for article in articles:
        article_date_str = article.get('date', '')
        article_date = parse_date(article_date_str)
        
        if not article_date:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
        if start_date and article_date < start_date:
            continue
        if end_date and article_date > end_date:
            continue
        
        filtered_articles.append(article)
    
    return filtered_articles

def get_preset_date_range(option):
    """è·å–é¢„è®¾çš„æ—¶é—´èŒƒå›´"""
    today = datetime.now()
    
    if option == "1":  # è¿‡å»ä¸€å¹´
        start_date = today - timedelta(days=365)
        return start_date, today, "è¿‡å»ä¸€å¹´"
    elif option == "2":  # è¿‡å»åŠå¹´
        start_date = today - timedelta(days=180)
        return start_date, today, "è¿‡å»åŠå¹´"
    elif option == "3":  # è¿‡å»ä¸‰ä¸ªæœˆ
        start_date = today - timedelta(days=90)
        return start_date, today, "è¿‡å»ä¸‰ä¸ªæœˆ"
    elif option == "4":  # è¿‡å»ä¸€ä¸ªæœˆ
        start_date = today - timedelta(days=30)
        return start_date, today, "è¿‡å»ä¸€ä¸ªæœˆ"
    elif option == "5":  # è¿‡å»ä¸€å‘¨
        start_date = today - timedelta(days=7)
        return start_date, today, "è¿‡å»ä¸€å‘¨"
    elif option == "6":  # ä»Šå¹´
        start_date = datetime(today.year, 1, 1)
        return start_date, today, "ä»Šå¹´"
    elif option == "7":  # å»å¹´
        start_date = datetime(today.year - 1, 1, 1)
        end_date = datetime(today.year - 1, 12, 31)
        return start_date, end_date, "å»å¹´"
    else:
        return None, None, None

def get_custom_date_range():
    """è·å–ç”¨æˆ·è‡ªå®šä¹‰çš„æ—¶é—´èŒƒå›´"""
    print("\nè¯·è¾“å…¥è‡ªå®šä¹‰æ—¶é—´èŒƒå›´ï¼š")
    
    while True:
        start_str = input("å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2024-01-01): ").strip()
        if not start_str:
            return None, None
        
        start_date = parse_date(start_str)
        if start_date:
            break
        else:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    while True:
        end_str = input("ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2024-12-31): ").strip()
        if not end_str:
            end_date = datetime.now()
            break
        
        end_date = parse_date(end_str)
        if end_date:
            break
        else:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    return start_date, end_date

def show_time_range_menu():
    """æ˜¾ç¤ºæ—¶é—´èŒƒå›´é€‰æ‹©èœå•"""
    print("\n" + "="*50)
    print("ğŸ“… è¯·é€‰æ‹©æŠ“å–èŒƒå›´ï¼š")
    print("="*50)
    print("æ—¶é—´èŒƒå›´é€‰é¡¹ï¼š")
    print("1. è¿‡å»ä¸€å¹´ (365å¤©)")
    print("2. è¿‡å»åŠå¹´ (180å¤©)")
    print("3. è¿‡å»ä¸‰ä¸ªæœˆ (90å¤©)")
    print("4. è¿‡å»ä¸€ä¸ªæœˆ (30å¤©)")
    print("5. è¿‡å»ä¸€å‘¨ (7å¤©)")
    print("6. ä»Šå¹´ (1æœˆ1æ—¥è‡³ä»Š)")
    print("7. å»å¹´ (å…¨å¹´)")
    print("8. è‡ªå®šä¹‰æ—¶é—´èŒƒå›´")
    print("-"*50)
    print("å¿«é€Ÿæµ‹è¯•é€‰é¡¹ï¼š")
    print("10. æœ€æ–°1ç¯‡æ–‡ç«  (å¿«é€Ÿæµ‹è¯•)")
    print("11. æœ€æ–°3ç¯‡æ–‡ç«  (åŸºæœ¬æµ‹è¯•)")
    print("12. æœ€æ–°10ç¯‡æ–‡ç«  (å®Œæ•´æµ‹è¯•)")
    print("13. è‡ªå®šä¹‰æ•°é‡")
    print("-"*50)
    print("å…¶ä»–é€‰é¡¹ï¼š")
    print("9. æŠ“å–æ‰€æœ‰æ–‡ç« ")
    print("0. é€€å‡ºç¨‹åº")
    print("="*50)

def get_latest_n_articles(articles, n):
    """è·å–æœ€æ–°çš„Nç¯‡æ–‡ç« """
    # æŒ‰æ—¥æœŸæ’åºï¼ˆå¦‚æœæœ‰æ—¥æœŸï¼‰
    dated_articles = []
    undated_articles = []
    
    for article in articles:
        date_str = article.get('date', '')
        date = parse_date(date_str)
        if date:
            dated_articles.append((date, article))
        else:
            undated_articles.append(article)
    
    # æŒ‰æ—¥æœŸé™åºæ’åº
    dated_articles.sort(key=lambda x: x[0], reverse=True)
    
    # è·å–å‰Nç¯‡æ–‡ç« 
    result = []
    
    # é¦–å…ˆæ·»åŠ æœ‰æ—¥æœŸçš„æ–‡ç« 
    for _, article in dated_articles[:n]:
        result.append(article)
    
    # å¦‚æœè¿˜ä¸å¤ŸNç¯‡ï¼Œæ·»åŠ æ— æ—¥æœŸçš„æ–‡ç« 
    remaining = n - len(result)
    if remaining > 0:
        result.extend(undated_articles[:remaining])
    
    actual_count = len(result)
    if actual_count < n:
        print(f"\nâš ï¸ è­¦å‘Šï¼šè¦æ±‚æŠ“å–{n}ç¯‡æ–‡ç« ï¼Œä½†åªæ‰¾åˆ°{actual_count}ç¯‡")
    
    return result

def get_custom_article_count():
    """è·å–ç”¨æˆ·è‡ªå®šä¹‰çš„æ–‡ç« æ•°é‡"""
    while True:
        try:
            count = input("\nè¯·è¾“å…¥è¦æŠ“å–çš„æ–‡ç« æ•°é‡: ").strip()
            count = int(count)
            if count <= 0:
                print("âŒ è¯·è¾“å…¥å¤§äº0çš„æ•°å­—")
                continue
            return count
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

def download_image(img_url: str, save_dir: str) -> str:
    """
    ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„
    """
    try:
        # è§£æURLï¼Œè·å–æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        img_name = os.path.basename(unquote(parsed_url.path))
        
        # ç¡®ä¿æ–‡ä»¶åæœ‰æ•ˆä¸”å”¯ä¸€
        img_name = re.sub(r'[<>:"/\\|?*]', '_', img_name)  # æ›¿æ¢éæ³•å­—ç¬¦
        if not img_name or len(img_name) > 255 or not re.search(r'\.(jpg|jpeg|png|gif|webp)$', img_name, re.I):
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ‰©å±•åï¼Œé»˜è®¤ä½¿ç”¨.jpg
            img_name = f"img_{int(time.time()*1000)}.jpg"
        
        # æ„å»ºä¿å­˜è·¯å¾„
        img_path = os.path.join(save_dir, img_name)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ•°å­—åç¼€
        base, ext = os.path.splitext(img_path)
        counter = 1
        while os.path.exists(img_path):
            img_path = f"{base}_{counter}{ext}"
            counter += 1
        
        # ä¸‹è½½å›¾ç‰‡
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(img_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶
        content_type = response.headers.get('content-type', '')
        if not content_type.startswith('image/'):
            raise Exception(f"Invalid content type: {content_type}")
        
        # ä¿å­˜å›¾ç‰‡
        with open(img_path, 'wb') as f:
            f.write(response.content)
        
        print(f"    âœ“ å›¾ç‰‡å·²ä¿å­˜: {os.path.basename(img_path)}")
        return img_path
    except Exception as e:
        print(f"    âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ ({img_url}): {e}")
        return None

def html_to_markdown(element, page, images_dir) -> tuple[str, list]:
    """
    å°†HTMLå…ƒç´ è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶è¿”å›å›¾ç‰‡ä¿¡æ¯
    è¿”å›: (markdown_content, images_info)
    """
    if not element:
        return "", []

    try:
        images_info = []  # å­˜å‚¨å›¾ç‰‡ä¿¡æ¯
        
        # é¦–å…ˆå¤„ç†æ‰€æœ‰å›¾ç‰‡å…ƒç´ 
        img_elements = element.query_selector_all('img')
        print(f"    æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")
        
        for img in img_elements:
            try:
                # è·å–å›¾ç‰‡URL
                img_url = img.get_attribute('data-src') or img.get_attribute('src')
                if img_url:
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif not img_url.startswith(('http://', 'https://')):
                        img_url = 'https://' + img_url
                    
                    print(f"    æ­£åœ¨å¤„ç†å›¾ç‰‡: {img_url}")
                    
                    # ä¸‹è½½å›¾ç‰‡
                    local_path = download_image(img_url, images_dir)
                    if local_path:
                        alt_text = img.get_attribute('alt') or 'å›¾ç‰‡'
                        images_info.append({
                            'original_url': img_url,
                            'local_path': local_path,
                            'alt_text': alt_text,
                            'filename': os.path.basename(local_path)
                        })
            except Exception as e:
                print(f"    âš ï¸ å¤„ç†å›¾ç‰‡å…ƒç´ å¤±è´¥: {e}")

        # è·å–å…ƒç´ çš„HTMLå†…å®¹
        html_content = element.evaluate("""element => {
            // å¤„ç†å›¾ç‰‡æ‡’åŠ è½½
            element.querySelectorAll('img').forEach(img => {
                if (img.dataset.src) {
                    img.src = img.dataset.src;
                }
            });
            return element.innerHTML;
        }""")
        
        # ä½¿ç”¨JavaScriptæå–æ ¼å¼åŒ–å†…å®¹
        formatted_content = element.evaluate("""element => {
            function getMarkdown(node, depth = 0) {
                if (!node) return '';
                
                let result = '';
                
                // å¤„ç†æ–‡æœ¬èŠ‚ç‚¹
                if (node.nodeType === Node.TEXT_NODE) {
                    let text = node.textContent.trim();
                    if (text) return text;
                    return '';
                }
                
                // å¤„ç†å…ƒç´ èŠ‚ç‚¹
                if (node.nodeType === Node.ELEMENT_NODE) {
                    let nodeName = node.nodeName.toLowerCase();
                    let classList = Array.from(node.classList || []);
                    
                    // è·³è¿‡æ ·å¼å’Œè„šæœ¬æ ‡ç­¾
                    if (['style', 'script'].includes(nodeName)) {
                        return '';
                    }
                    
                    // è·å–æ‰€æœ‰å­èŠ‚ç‚¹çš„å†…å®¹
                    let childContent = Array.from(node.childNodes)
                        .map(child => getMarkdown(child, depth + 1))
                        .filter(text => text)
                        .join(' ')
                        .trim();
                    
                    if (!childContent) return '';
                    
                    // å¤„ç†ä¸åŒç±»å‹çš„å…ƒç´ 
                    switch (nodeName) {
                        case 'h1': return `\\n# ${childContent}\\n`;
                        case 'h2': return `\\n## ${childContent}\\n`;
                        case 'h3': return `\\n### ${childContent}\\n`;
                        case 'h4': return `\\n#### ${childContent}\\n`;
                        case 'p': return `\\n${childContent}\\n`;
                        case 'strong':
                        case 'b': return `**${childContent}**`;
                        case 'em':
                        case 'i': return `*${childContent}*`;
                        case 'code': return `\`${childContent}\``;
                        case 'pre': return `\\n\`\`\`\\n${childContent}\\n\`\`\`\\n`;
                        case 'blockquote': return `\\n> ${childContent}\\n`;
                        case 'a': return `[${childContent}](${node.href || ''})`;
                        case 'img': {
                            let src = node.src || node.dataset.src;
                            let alt = node.alt || 'å›¾ç‰‡';
                            return src ? `\\n![${alt}](${src})\\n` : '';
                        }
                        case 'ul': {
                            return '\\n' + Array.from(node.children)
                                .map(li => `- ${getMarkdown(li).trim()}`)
                                .join('\\n') + '\\n';
                        }
                        case 'ol': {
                            return '\\n' + Array.from(node.children)
                                .map((li, i) => `${i + 1}. ${getMarkdown(li).trim()}`)
                                .join('\\n') + '\\n';
                        }
                        case 'li': return childContent;
                        case 'br': return '\\n';
                        default: return childContent;
                    }
                }
                
                return '';
            }
            
            return getMarkdown(element);
        }""")
        
        # å¤„ç†JavaScriptè½¬ä¹‰çš„æ¢è¡Œç¬¦
        formatted_content = formatted_content.replace('\\n', '\n')
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        formatted_content = re.sub(r'\n{3,}', '\n\n', formatted_content)
        
        print(f"    âœ“ æˆåŠŸå¤„ç† {len(images_info)} å¼ å›¾ç‰‡")
        return formatted_content.strip(), images_info
        
    except Exception as e:
        print(f"    âš ï¸ Markdownè½¬æ¢å‡ºé”™: {str(e)}")
        return "", []

def fetch_article_content(url, folder_name, retry_count=5):
    """æŠ“å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ”¯æŒé‡è¯•"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()
        
        # æ³¨å…¥è¾…åŠ©å‡½æ•°
        page.add_init_script("""
            window.getComputedStyle = window.getComputedStyle || function(element) {
                return element.currentStyle;
            };
        """)
        
        # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        page.set_default_timeout(45000)
        
        for attempt in range(retry_count):
            try:
                print(f"    å°è¯•ç¬¬ {attempt + 1} æ¬¡è®¿é—®...")
                page.goto(url, timeout=45000)
                
                # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
                page.wait_for_load_state('domcontentloaded', timeout=20000)
                
                # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
                page.wait_for_selector('div#js_content', timeout=20000)
                
                # é¢å¤–ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿å†…å®¹åŠ è½½
                page.wait_for_timeout(5000)
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ•ˆï¼ˆæ–‡ç« æ˜¯å¦å­˜åœ¨ï¼‰
                page_content = page.content()
                
                # æ£€æµ‹åˆ é™¤/è¿è§„é¡µé¢çš„å…³é”®è¯
                invalid_keywords = [
                    "æ­¤å†…å®¹å‘é€å¤±è´¥æ— æ³•æŸ¥çœ‹",
                    "æ­¤å†…å®¹å› æ¶‰å«Œè¿åç›¸å…³æ³•å¾‹æ³•è§„å’Œæ”¿ç­–å‘é€å¤±è´¥",
                    "å†…å®¹å·²åˆ é™¤",
                    "æ–‡ç« ä¸å­˜åœ¨",
                    "è¯¥å†…å®¹å·²è¢«åˆ é™¤",
                    "å†…å®¹è¿è§„",
                    "æ— æ³•æŸ¥çœ‹",
                    "å‘é€å¤±è´¥",
                    "è¿è§„å†…å®¹",
                    "å†…å®¹ä¸å­˜åœ¨"
                ]
                
                is_invalid_page = any(keyword in page_content for keyword in invalid_keywords)
                
                if is_invalid_page:
                    print(f"    âš ï¸  æ£€æµ‹åˆ°æ— æ•ˆé¡µé¢ï¼Œæ–‡ç« å¯èƒ½å·²è¢«åˆ é™¤æˆ–è¿è§„")
                    browser.close()
                    return {
                        'url': url,
                        'title': '',
                        'author': '',
                        'publish_time': '',
                        'read_count': '',
                        'like_count': '',
                        'content': '',
                        'error': 'æ–‡ç« å·²è¢«åˆ é™¤æˆ–è¿è§„ï¼Œæ— æ³•æŸ¥çœ‹',
                        'status': 'deleted'
                    }
                
                # æå–æ–‡ç« ä¿¡æ¯
                article_data = {
                    'url': url,
                    'title': '',
                    'author': '',
                    'publish_time': '',
                    'read_count': '',
                    'like_count': '',
                    'content': '',
                    'content_format': 'markdown',  # æ ‡è®°å†…å®¹æ ¼å¼
                    'images': [],  # è®°å½•å›¾ç‰‡ä¿¡æ¯
                    'metadata': {
                        'crawl_time': datetime.now().isoformat(),
                        'markdown_enabled': True,
                        'images_saved': False,
                        'version': '1.0'
                    }
                }
                
                # æå–æ ‡é¢˜
                try:
                    title_element = page.query_selector('h1.rich_media_title')
                    if title_element:
                        article_data['title'] = title_element.inner_text().strip()
                except:
                    pass
                
                # æå–ä½œè€…
                try:
                    author_element = page.query_selector('a#js_name')
                    if author_element:
                        article_data['author'] = author_element.inner_text().strip()
                except:
                    pass
                
                # æå–å‘å¸ƒæ—¶é—´
                try:
                    time_element = page.query_selector('em#publish_time')
                    if time_element:
                        article_data['publish_time'] = time_element.inner_text().strip()
                except:
                    pass
                
                # æå–é˜…è¯»é‡
                try:
                    read_element = page.query_selector('span#js_read_area')
                    if read_element:
                        read_text = read_element.inner_text().strip()
                        # æå–æ•°å­—
                        read_match = re.search(r'(\d+)', read_text)
                        if read_match:
                            article_data['read_count'] = read_match.group(1)
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        read_elements = page.query_selector_all('span')
                        for element in read_elements:
                            text = element.inner_text().strip()
                            if 'é˜…è¯»' in text and re.search(r'\d+', text):
                                read_match = re.search(r'(\d+)', text)
                                if read_match:
                                    article_data['read_count'] = read_match.group(1)
                                    break
                except:
                    pass
                
                # æå–ç‚¹èµé‡
                try:
                    like_element = page.query_selector('span#like_area')
                    if like_element:
                        like_text = like_element.inner_text().strip()
                        like_match = re.search(r'(\d+)', like_text)
                        if like_match:
                            article_data['like_count'] = like_match.group(1)
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        like_elements = page.query_selector_all('span')
                        for element in like_elements:
                            text = element.inner_text().strip()
                            if 'èµ' in text and re.search(r'\d+', text):
                                like_match = re.search(r'(\d+)', text)
                                if like_match:
                                    article_data['like_count'] = like_match.group(1)
                                    break
                except:
                    pass
                
                # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
                images_dir = os.path.join(folder_name, 'images')
                os.makedirs(images_dir, exist_ok=True)
                article_data['images_dir'] = os.path.relpath(images_dir, folder_name)  # ä¿å­˜ç›¸å¯¹è·¯å¾„

                # æå–æ­£æ–‡å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
                try:
                    content_element = page.query_selector('div#js_content')
                    if content_element:
                        # å°†å†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼
                        content, images = html_to_markdown(content_element, page, images_dir)
                        article_data['content'] = content
                        article_data['images'] = images
                        
                        if not content:
                            raise Exception("å†…å®¹è½¬æ¢åä¸ºç©º")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡è¢«ä¿å­˜
                        if images:
                            article_data['metadata']['images_saved'] = True
                            article_data['metadata']['image_count'] = len(images)
                except Exception as e:
                    print(f"    âš ï¸ å†…å®¹è½¬æ¢å¤±è´¥: {e}")
                    # å°è¯•åŸºæœ¬çš„æ–‡æœ¬æå–
                    try:
                        content_element = page.query_selector('div#js_content')
                        if content_element:
                            article_data['content'] = content_element.inner_text().strip()
                            article_data['content_format'] = 'plain'
                            article_data['metadata']['markdown_enabled'] = False
                    except:
                        pass
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸæŠ“å–åˆ°æœ‰æ•ˆå†…å®¹
                if article_data.get('title') and len(article_data.get('title', '').strip()) > 0:
                    browser.close()
                    return article_data
                else:
                    # å¦‚æœæ²¡æœ‰æŠ“å–åˆ°æ ‡é¢˜ï¼Œç»§ç»­é‡è¯•
                    raise Exception("æœªæŠ“å–åˆ°æ–‡ç« æ ‡é¢˜ï¼Œå¯èƒ½é¡µé¢æœªå®Œå…¨åŠ è½½")
                
            except Exception as e:
                print(f"    ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < retry_count - 1:
                    # é€’å¢ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    wait_time = (attempt + 1) * 3
                    print(f"    ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"    æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    browser.close()
                    return {
                        'url': url,
                        'title': '',
                        'author': '',
                        'publish_time': '',
                        'read_count': '',
                        'like_count': '',
                        'content': '',
                        'error': f"é‡è¯• {retry_count} æ¬¡åä»ç„¶å¤±è´¥: {str(e)}"
                    }

def main():
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡æŠ“å–å·¥å…· (JSON ç‰ˆ)")
    print("ä» ArticleList.json æ–‡ä»¶è¯»å–æ–‡ç« é“¾æ¥å¹¶æ‰¹é‡æŠ“å–")
    
    # è·å–å½“å‰å·¥ä½œç›®å½•ï¼ˆåº”è¯¥æ˜¯æ ¹ç›®å½•ä¸‹çš„æŸä¸ªå­ç›®å½•ï¼‰
    current_dir = os.getcwd()
    
    # è¯»å– ArticleList.json æ–‡ä»¶
    try:
        with open("ArticleList.json", "r", encoding="utf-8") as f:
            all_articles = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å– ArticleList.jsonï¼Œå…± {len(all_articles)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"âŒ è¯»å– ArticleList.json å¤±è´¥: {e}")
        return

    # æ˜¾ç¤ºæ—¶é—´èŒƒå›´é€‰æ‹©èœå•
    while True:
        show_time_range_menu()
        choice = input("è¯·é€‰æ‹© (0-13): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")
            return
        
        elif choice == "9":
            # æŠ“å–æ‰€æœ‰æ–‡ç« 
            start_date = None
            end_date = None
            filtered_articles = all_articles
            range_name = "æ‰€æœ‰æ–‡ç« "
            break
        
        elif choice == "8":
            # è‡ªå®šä¹‰æ—¶é—´èŒƒå›´
            start_date, end_date = get_custom_date_range()
            if start_date and end_date:
                filtered_articles = filter_articles_by_date(all_articles, start_date, end_date)
                range_name = f"è‡ªå®šä¹‰èŒƒå›´ ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})"
                break
            else:
                print("âŒ è‡ªå®šä¹‰æ—¶é—´èŒƒå›´è®¾ç½®å¤±è´¥")
                continue
        
        elif choice in ["1", "2", "3", "4", "5", "6", "7"]:
            # é¢„è®¾æ—¶é—´èŒƒå›´
            start_date, end_date, range_name = get_preset_date_range(choice)
            if start_date and end_date:
                filtered_articles = filter_articles_by_date(all_articles, start_date, end_date)
                break
            else:
                print("âŒ é¢„è®¾æ—¶é—´èŒƒå›´è·å–å¤±è´¥")
                continue
        
        elif choice == "10":
            # æœ€æ–°1ç¯‡ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
            filtered_articles = get_latest_n_articles(all_articles, 1)
            range_name = "æœ€æ–°1ç¯‡æ–‡ç«  (å¿«é€Ÿæµ‹è¯•)"
            start_date = end_date = None
            break
            
        elif choice == "11":
            # æœ€æ–°3ç¯‡ï¼ˆåŸºæœ¬æµ‹è¯•ï¼‰
            filtered_articles = get_latest_n_articles(all_articles, 3)
            range_name = "æœ€æ–°3ç¯‡æ–‡ç«  (åŸºæœ¬æµ‹è¯•)"
            start_date = end_date = None
            break
            
        elif choice == "12":
            # æœ€æ–°10ç¯‡ï¼ˆå®Œæ•´æµ‹è¯•ï¼‰
            filtered_articles = get_latest_n_articles(all_articles, 10)
            range_name = "æœ€æ–°10ç¯‡æ–‡ç«  (å®Œæ•´æµ‹è¯•)"
            start_date = end_date = None
            break
            
        elif choice == "13":
            # è‡ªå®šä¹‰æ•°é‡
            count = get_custom_article_count()
            filtered_articles = get_latest_n_articles(all_articles, count)
            range_name = f"æœ€æ–°{count}ç¯‡æ–‡ç« "
            start_date = end_date = None
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
    
    # æ˜¾ç¤ºç­›é€‰ç»“æœ
    print(f"\nğŸ“Š æŠ“å–èŒƒå›´: {range_name}")
    if start_date or end_date:
        print(f"ğŸ“… ç­›é€‰æ¡ä»¶: {start_date.strftime('%Y-%m-%d') if start_date else 'ä¸é™'} è‡³ {end_date.strftime('%Y-%m-%d') if end_date else 'ä¸é™'}")
    print(f"ğŸ“ ç­›é€‰ç»“æœ: {len(filtered_articles)} ç¯‡æ–‡ç« ")
    
    if len(filtered_articles) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
        return
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    confirm = input(f"\næ˜¯å¦å¼€å§‹æŠ“å–è¿™ {len(filtered_articles)} ç¯‡æ–‡ç« ï¼Ÿ(y/n): ").strip().lower()
    if confirm not in ['y', 'yes', 'æ˜¯']:
        print("ğŸ‘‹ å·²å–æ¶ˆæŠ“å–")
        return

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆåœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    batch_folder = f"articles_batch_{timestamp}"
    
    # ç¡®ä¿æ–‡ä»¶å¤¹åç§°å”¯ä¸€
    counter = 1
    original_folder_name = batch_folder
    while os.path.exists(batch_folder):
        batch_folder = f"{original_folder_name}_{counter}"
        counter += 1
    
    # åˆ›å»ºä¸»æ–‡ä»¶å¤¹å’Œå›¾ç‰‡æ–‡ä»¶å¤¹ï¼ˆåœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹ï¼‰
    os.makedirs(batch_folder, exist_ok=True)
    batch_images_dir = os.path.join(batch_folder, 'images')
    os.makedirs(batch_images_dir, exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {batch_folder}")
    print(f"ğŸ“ åˆ›å»ºå›¾ç‰‡æ–‡ä»¶å¤¹: {batch_images_dir}")

    # æå–æ‰€æœ‰é“¾æ¥
    urls = []
    for item in filtered_articles:
        if 'link' in item:
            urls.append(item['link'])
        elif 'url' in item:
            urls.append(item['url'])

    # æ‰¹é‡æŠ“å–æ–‡ç« 
    articles = []
    for idx, url in enumerate(urls, 1):
        print(f"\n[{idx}/{len(urls)}] æ­£åœ¨æŠ“å–: {url}")
        
        try:
            # ç¡®ä¿æ¯ç¯‡æ–‡ç« éƒ½ä½¿ç”¨æ­£ç¡®çš„å›¾ç‰‡ä¿å­˜è·¯å¾„
            article_data = fetch_article_content(url, batch_folder)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"    å›¾ç‰‡ä¿å­˜ç›®å½•: {batch_images_dir}")
            print(f"    æ–‡ç« å›¾ç‰‡æ•°é‡: {len(article_data.get('images', []))}")
            
            articles.append(article_data)
            
            # æ˜¾ç¤ºæŠ“å–ç»“æœ
            if article_data.get('title'):
                print(f"    âœ… æˆåŠŸ: {article_data['title'][:50]}...")
                if article_data.get('metadata', {}).get('images_saved'):
                    print(f"       ğŸ“¸ å·²ä¿å­˜ {article_data.get('metadata', {}).get('image_count', 0)} å¼ å›¾ç‰‡")
            else:
                print(f"    âŒ å¤±è´¥: æœªè·å–åˆ°æ ‡é¢˜")
            
        except Exception as e:
            print(f"    âŒ æŠ“å–å¼‚å¸¸: {e}")
            articles.append({
                'url': url,
                'title': '',
                'author': '',
                'publish_time': '',
                'read_count': '',
                'like_count': '',
                'content': '',
                'error': str(e),
                'content_format': 'plain',
                'images': [],
                'metadata': {
                    'crawl_time': datetime.now().isoformat(),
                    'markdown_enabled': False,
                    'images_saved': False,
                    'version': '1.0'
                }
            })
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶å¤¹
        if os.path.exists(batch_images_dir):
            image_files = os.listdir(batch_images_dir)
            print(f"    ğŸ“ å›¾ç‰‡æ–‡ä»¶å¤¹çŠ¶æ€: {len(image_files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"    âš ï¸ å›¾ç‰‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {batch_images_dir}")
        
        # é˜²æ­¢è¿‡å¿«è¢«å°ï¼Œæ¯æ¬¡æŠ“å–é—´éš” 5 ç§’
        time.sleep(5)

    # ä¿å­˜ç»“æœæ–‡ä»¶
    output_file = os.path.join(batch_folder, "articles_detailed.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for article in articles if article.get('title'))
    fail_count = len(articles) - success_count
    success_rate = success_count/len(articles)*100 if len(articles) > 0 else 0
    
    # åˆ†æå¤±è´¥åŸå› 
    failed_articles = [article for article in articles if not article.get('title')]
    deleted_articles = [article for article in articles if article.get('status') == 'deleted']
    error_analysis = {}
    
    for article in failed_articles:
        error_msg = article.get('error', 'æœªçŸ¥é”™è¯¯')
        error_analysis[error_msg] = error_analysis.get(error_msg, 0) + 1
    
    # ä¿å­˜æŠ“å–ä¿¡æ¯
    info_file = os.path.join(batch_folder, "crawl_info.json")
    crawl_info = {
        "crawl_time": datetime.now().isoformat(),
        "time_range": range_name,
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "total_articles": len(articles),
        "success_count": success_count,
        "fail_count": fail_count,
        "deleted_count": len(deleted_articles),
        "success_rate": f"{success_rate:.1f}%",
        "source_file": "ArticleList.json",
        "error_analysis": error_analysis,
        "format_version": "1.0",
        "markdown_enabled": True,
        "image_support": True,
        "images_dir": "images"
    }
    
    with open(info_file, "w", encoding="utf-8") as f:
        json.dump(crawl_info, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {batch_folder}")
    print(f"   ğŸ“„ æ–‡ç« æ•°æ®: {output_file}")
    print(f"   ğŸ“‹ æŠ“å–ä¿¡æ¯: {info_file}")
    print(f"   ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {os.path.join(batch_folder, 'images')}")

if __name__ == "__main__":
    main() 