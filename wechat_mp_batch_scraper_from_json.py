import json
import time
import re
import os
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright

def parse_date(date_str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if not date_str:
        return None
    
    date_formats = [
        '%Y-%m-%d',
        '%Yå¹´%mæœˆ%dæ—¥',
        '%Y/%m/%d',
        '%Y.%m.%d'
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    
    return None

def filter_articles_by_date(articles, start_date=None, end_date=None):
    """æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ç« """
    filtered_articles = []
    
    for article in articles:
        article_date_str = article.get('date', '')
        article_date = parse_date(article_date_str)
        
        if not article_date:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
        if start_date and article_date < start_date:
            continue
        if end_date and article_date > end_date:
            continue
        
        filtered_articles.append(article)
    
    return filtered_articles

def get_preset_date_range(option):
    """è·å–é¢„è®¾çš„æ—¶é—´èŒƒå›´"""
    today = datetime.now()
    
    if option == "1":  # è¿‡å»ä¸€å¹´
        start_date = today - timedelta(days=365)
        return start_date, today, "è¿‡å»ä¸€å¹´"
    elif option == "2":  # è¿‡å»åŠå¹´
        start_date = today - timedelta(days=180)
        return start_date, today, "è¿‡å»åŠå¹´"
    elif option == "3":  # è¿‡å»ä¸‰ä¸ªæœˆ
        start_date = today - timedelta(days=90)
        return start_date, today, "è¿‡å»ä¸‰ä¸ªæœˆ"
    elif option == "4":  # è¿‡å»ä¸€ä¸ªæœˆ
        start_date = today - timedelta(days=30)
        return start_date, today, "è¿‡å»ä¸€ä¸ªæœˆ"
    elif option == "5":  # è¿‡å»ä¸€å‘¨
        start_date = today - timedelta(days=7)
        return start_date, today, "è¿‡å»ä¸€å‘¨"
    elif option == "6":  # ä»Šå¹´
        start_date = datetime(today.year, 1, 1)
        return start_date, today, "ä»Šå¹´"
    elif option == "7":  # å»å¹´
        start_date = datetime(today.year - 1, 1, 1)
        end_date = datetime(today.year - 1, 12, 31)
        return start_date, end_date, "å»å¹´"
    else:
        return None, None, None

def get_custom_date_range():
    """è·å–ç”¨æˆ·è‡ªå®šä¹‰çš„æ—¶é—´èŒƒå›´"""
    print("\nè¯·è¾“å…¥è‡ªå®šä¹‰æ—¶é—´èŒƒå›´ï¼š")
    
    while True:
        start_str = input("å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2024-01-01): ").strip()
        if not start_str:
            return None, None
        
        start_date = parse_date(start_str)
        if start_date:
            break
        else:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    while True:
        end_str = input("ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2024-12-31): ").strip()
        if not end_str:
            end_date = datetime.now()
            break
        
        end_date = parse_date(end_str)
        if end_date:
            break
        else:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    return start_date, end_date

def show_time_range_menu():
    """æ˜¾ç¤ºæ—¶é—´èŒƒå›´é€‰æ‹©èœå•"""
    print("\n" + "="*50)
    print("ğŸ“… è¯·é€‰æ‹©æŠ“å–æ–‡ç« çš„æ—¶é—´èŒƒå›´ï¼š")
    print("="*50)
    print("1. è¿‡å»ä¸€å¹´ (365å¤©)")
    print("2. è¿‡å»åŠå¹´ (180å¤©)")
    print("3. è¿‡å»ä¸‰ä¸ªæœˆ (90å¤©)")
    print("4. è¿‡å»ä¸€ä¸ªæœˆ (30å¤©)")
    print("5. è¿‡å»ä¸€å‘¨ (7å¤©)")
    print("6. ä»Šå¹´ (1æœˆ1æ—¥è‡³ä»Š)")
    print("7. å»å¹´ (å…¨å¹´)")
    print("8. è‡ªå®šä¹‰æ—¶é—´èŒƒå›´")
    print("9. æŠ“å–æ‰€æœ‰æ–‡ç« ")
    print("0. é€€å‡ºç¨‹åº")
    print("="*50)

def fetch_article_content(url, retry_count=5):
    """æŠ“å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ”¯æŒé‡è¯•"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        page.set_default_timeout(45000)
        
        for attempt in range(retry_count):
            try:
                print(f"    å°è¯•ç¬¬ {attempt + 1} æ¬¡è®¿é—®...")
                page.goto(url, timeout=45000)
                
                # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
                page.wait_for_load_state('domcontentloaded', timeout=20000)
                
                # é¢å¤–ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿å†…å®¹åŠ è½½
                page.wait_for_timeout(5000)
                
                # æå–æ–‡ç« ä¿¡æ¯
                article_data = {
                    'url': url,
                    'title': '',
                    'author': '',
                    'publish_time': '',
                    'read_count': '',
                    'like_count': '',
                    'content': ''
                }
                
                # æå–æ ‡é¢˜
                try:
                    title_element = page.query_selector('h1.rich_media_title')
                    if title_element:
                        article_data['title'] = title_element.inner_text().strip()
                except:
                    pass
                
                # æå–ä½œè€…
                try:
                    author_element = page.query_selector('a#js_name')
                    if author_element:
                        article_data['author'] = author_element.inner_text().strip()
                except:
                    pass
                
                # æå–å‘å¸ƒæ—¶é—´
                try:
                    time_element = page.query_selector('em#publish_time')
                    if time_element:
                        article_data['publish_time'] = time_element.inner_text().strip()
                except:
                    pass
                
                # æå–é˜…è¯»é‡
                try:
                    read_element = page.query_selector('span#js_read_area')
                    if read_element:
                        read_text = read_element.inner_text().strip()
                        # æå–æ•°å­—
                        read_match = re.search(r'(\d+)', read_text)
                        if read_match:
                            article_data['read_count'] = read_match.group(1)
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        read_elements = page.query_selector_all('span')
                        for element in read_elements:
                            text = element.inner_text().strip()
                            if 'é˜…è¯»' in text and re.search(r'\d+', text):
                                read_match = re.search(r'(\d+)', text)
                                if read_match:
                                    article_data['read_count'] = read_match.group(1)
                                    break
                except:
                    pass
                
                # æå–ç‚¹èµé‡
                try:
                    like_element = page.query_selector('span#like_area')
                    if like_element:
                        like_text = like_element.inner_text().strip()
                        like_match = re.search(r'(\d+)', like_text)
                        if like_match:
                            article_data['like_count'] = like_match.group(1)
                    else:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        like_elements = page.query_selector_all('span')
                        for element in like_elements:
                            text = element.inner_text().strip()
                            if 'èµ' in text and re.search(r'\d+', text):
                                like_match = re.search(r'(\d+)', text)
                                if like_match:
                                    article_data['like_count'] = like_match.group(1)
                                    break
                except:
                    pass
                
                # æå–æ­£æ–‡å†…å®¹
                try:
                    content_element = page.query_selector('div#js_content')
                    if content_element:
                        article_data['content'] = content_element.inner_text().strip()
                except:
                    pass
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸæŠ“å–åˆ°æœ‰æ•ˆå†…å®¹
                if article_data.get('title') and len(article_data.get('title', '').strip()) > 0:
                    browser.close()
                    return article_data
                else:
                    # å¦‚æœæ²¡æœ‰æŠ“å–åˆ°æ ‡é¢˜ï¼Œç»§ç»­é‡è¯•
                    raise Exception("æœªæŠ“å–åˆ°æ–‡ç« æ ‡é¢˜ï¼Œå¯èƒ½é¡µé¢æœªå®Œå…¨åŠ è½½")
                
            except Exception as e:
                print(f"    ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < retry_count - 1:
                    # é€’å¢ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    wait_time = (attempt + 1) * 3
                    print(f"    ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"    æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    browser.close()
                    return {
                        'url': url,
                        'title': '',
                        'author': '',
                        'publish_time': '',
                        'read_count': '',
                        'like_count': '',
                        'content': '',
                        'error': f"é‡è¯• {retry_count} æ¬¡åä»ç„¶å¤±è´¥: {str(e)}"
                    }
                
            except Exception as e:
                print(f"    ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < retry_count - 1:
                    # é€’å¢ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    wait_time = (attempt + 1) * 3
                    print(f"    ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"    æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    browser.close()
                    return {
                        'url': url,
                        'title': '',
                        'author': '',
                        'publish_time': '',
                        'read_count': '',
                        'like_count': '',
                        'content': '',
                        'error': f"é‡è¯• {retry_count} æ¬¡åä»ç„¶å¤±è´¥: {str(e)}"
                    }

def main():
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡æŠ“å–å·¥å…· (JSON ç‰ˆ)")
    print("ä» ArticleList.json æ–‡ä»¶è¯»å–æ–‡ç« é“¾æ¥å¹¶æ‰¹é‡æŠ“å–")
    
    # è¯»å– ArticleList.json æ–‡ä»¶
    try:
        with open("ArticleList.json", "r", encoding="utf-8") as f:
            all_articles = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å– ArticleList.jsonï¼Œå…± {len(all_articles)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"âŒ è¯»å– ArticleList.json å¤±è´¥: {e}")
        return
    
    # æ˜¾ç¤ºæ—¶é—´èŒƒå›´é€‰æ‹©èœå•
    while True:
        show_time_range_menu()
        choice = input("è¯·é€‰æ‹© (0-9): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")
            return
        
        elif choice == "9":
            # æŠ“å–æ‰€æœ‰æ–‡ç« 
            start_date = None
            end_date = None
            range_name = "æ‰€æœ‰æ–‡ç« "
            break
        
        elif choice == "8":
            # è‡ªå®šä¹‰æ—¶é—´èŒƒå›´
            start_date, end_date = get_custom_date_range()
            if start_date and end_date:
                range_name = f"è‡ªå®šä¹‰èŒƒå›´ ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})"
                break
            else:
                print("âŒ è‡ªå®šä¹‰æ—¶é—´èŒƒå›´è®¾ç½®å¤±è´¥")
                continue
        
        elif choice in ["1", "2", "3", "4", "5", "6", "7"]:
            # é¢„è®¾æ—¶é—´èŒƒå›´
            start_date, end_date, range_name = get_preset_date_range(choice)
            if start_date and end_date:
                break
            else:
                print("âŒ é¢„è®¾æ—¶é—´èŒƒå›´è·å–å¤±è´¥")
                continue
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
    
    # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ç« 
    if start_date or end_date:
        filtered_articles = filter_articles_by_date(all_articles, start_date, end_date)
        print(f"\nğŸ“Š æ—¶é—´èŒƒå›´: {range_name}")
        print(f"ğŸ“… ç­›é€‰æ¡ä»¶: {start_date.strftime('%Y-%m-%d') if start_date else 'ä¸é™'} è‡³ {end_date.strftime('%Y-%m-%d') if end_date else 'ä¸é™'}")
        print(f"ğŸ“ ç­›é€‰ç»“æœ: {len(filtered_articles)} ç¯‡æ–‡ç« ")
        
        if len(filtered_articles) == 0:
            print("âŒ è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
            return
        
        # ç¡®è®¤æ˜¯å¦ç»§ç»­
        confirm = input(f"\næ˜¯å¦å¼€å§‹æŠ“å–è¿™ {len(filtered_articles)} ç¯‡æ–‡ç« ï¼Ÿ(y/n): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("ğŸ‘‹ å·²å–æ¶ˆæŠ“å–")
            return
    else:
        filtered_articles = all_articles
        print(f"\nğŸ“Š æ—¶é—´èŒƒå›´: {range_name}")
        print(f"ğŸ“ å°†æŠ“å–æ‰€æœ‰ {len(filtered_articles)} ç¯‡æ–‡ç« ")
    
    # æå–æ‰€æœ‰é“¾æ¥
    urls = []
    for item in filtered_articles:
        if 'link' in item:
            urls.append(item['link'])
        elif 'url' in item:
            urls.append(item['url'])
    
    print(f"\nğŸš€ å¼€å§‹æŠ“å– {len(urls)} ç¯‡æ–‡ç« ...")
    
    # æ‰¹é‡æŠ“å–æ–‡ç« 
    articles = []
    for idx, url in enumerate(urls, 1):
        print(f"[{idx}/{len(urls)}] æ­£åœ¨æŠ“å–: {url}")
        
        try:
            article_data = fetch_article_content(url)
            articles.append(article_data)
            
            # æ˜¾ç¤ºæŠ“å–ç»“æœ
            if article_data.get('title'):
                print(f"  âœ… æˆåŠŸ: {article_data['title'][:50]}...")
            else:
                print(f"  âŒ å¤±è´¥: æœªè·å–åˆ°æ ‡é¢˜")
            
        except Exception as e:
            print(f"  âŒ æŠ“å–å¼‚å¸¸: {e}")
            articles.append({
                'url': url,
                'title': '',
                'author': '',
                'publish_time': '',
                'read_count': '',
                'like_count': '',
                'content': '',
                'error': str(e)
            })
        
        # é˜²æ­¢è¿‡å¿«è¢«å°ï¼Œæ¯æ¬¡æŠ“å–é—´éš” 5 ç§’
        time.sleep(5)
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"articles_batch_{timestamp}"
    
    # ç¡®ä¿æ–‡ä»¶å¤¹åç§°å”¯ä¸€
    counter = 1
    original_folder_name = folder_name
    while os.path.exists(folder_name):
        folder_name = f"{original_folder_name}_{counter}"
        counter += 1
    
    # åˆ›å»ºæ–‡ä»¶å¤¹
    os.makedirs(folder_name, exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {folder_name}")
    
    # ä¿å­˜ç»“æœæ–‡ä»¶
    output_file = os.path.join(folder_name, "articles_detailed.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for article in articles if article.get('title'))
    fail_count = len(articles) - success_count
    success_rate = success_count/len(articles)*100 if len(articles) > 0 else 0
    
    # åˆ†æå¤±è´¥åŸå› 
    failed_articles = [article for article in articles if not article.get('title')]
    error_analysis = {}
    for article in failed_articles:
        error_msg = article.get('error', 'æœªçŸ¥é”™è¯¯')
        error_analysis[error_msg] = error_analysis.get(error_msg, 0) + 1
    
    # ä¿å­˜æŠ“å–ä¿¡æ¯
    info_file = os.path.join(folder_name, "crawl_info.json")
    crawl_info = {
        "crawl_time": datetime.now().isoformat(),
        "time_range": range_name,
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "total_articles": len(articles),
        "success_count": success_count,
        "fail_count": fail_count,
        "success_rate": f"{success_rate:.1f}%",
        "source_file": "ArticleList.json",
        "error_analysis": error_analysis
    }
    with open(info_file, "w", encoding="utf-8") as f:
        json.dump(crawl_info, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ—¶é—´èŒƒå›´: {range_name}")
    print(f"   æ€»è®¡: {len(articles)} ç¯‡æ–‡ç« ")
    print(f"   æˆåŠŸ: {success_count} ç¯‡")
    print(f"   å¤±è´¥: {fail_count} ç¯‡")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
    
    # æ˜¾ç¤ºå¤±è´¥åˆ†æ
    if fail_count > 0:
        print(f"\nâŒ å¤±è´¥åˆ†æ:")
        for error_msg, count in error_analysis.items():
            print(f"   {error_msg}: {count} ç¯‡")
        
        # æä¾›æ”¹è¿›å»ºè®®
        if success_rate < 80:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            print(f"   - æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æŠ“å–é—´éš”æ—¶é—´")
            print(f"   - å¯ä»¥å°è¯•åœ¨ç½‘ç»œè¾ƒå¥½çš„æ—¶æ®µè¿›è¡ŒæŠ“å–")
            print(f"   - è€ƒè™‘åˆ†æ‰¹æŠ“å–ï¼Œå‡å°‘å•æ¬¡æŠ“å–æ•°é‡")
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {folder_name}")
    print(f"   ğŸ“„ æ–‡ç« æ•°æ®: {output_file}")
    print(f"   ğŸ“‹ æŠ“å–ä¿¡æ¯: {info_file}")

if __name__ == "__main__":
    main() 