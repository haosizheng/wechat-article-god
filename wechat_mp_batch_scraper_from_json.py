import os
import json
import time
from datetime import datetime
from utils.date_utils import get_preset_date_range, get_custom_date_range
from utils.text_utils import filter_articles_by_date, get_latest_n_articles
from utils.ui_utils import show_time_range_menu, show_crawl_options, get_custom_article_count
from utils.article_scraper import fetch_article_content

def process_single_list(json_file: str, output_base_dir: str, 
                     start_date=None, end_date=None, save_images=False, 
                     latest_n=None) -> None:
    """
    å¤„ç†å•ä¸ªæ–‡ç« åˆ—è¡¨æ–‡ä»¶
    Args:
        json_file: JSONæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        output_base_dir: è¾“å‡ºç›®å½•çš„åŸºç¡€è·¯å¾„ï¼ˆOutputæ–‡ä»¶å¤¹ï¼‰
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        save_images: æ˜¯å¦ä¿å­˜å›¾ç‰‡
        latest_n: å¦‚æœè®¾ç½®ï¼Œåˆ™åªå¤„ç†æœ€æ–°çš„Nç¯‡æ–‡ç« 
    """
    print(f"\nå¤„ç†æ–‡ç« åˆ—è¡¨: {os.path.basename(json_file)}")
    
    # è¯»å–æ–‡ç« åˆ—è¡¨æ–‡ä»¶
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            all_articles = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å–æ–‡ç« åˆ—è¡¨ï¼Œå…± {len(all_articles)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ç« 
    if start_date or end_date:
        filtered_articles = filter_articles_by_date(all_articles, start_date, end_date)
        date_range = f"({start_date.strftime('%Y-%m-%d') if start_date else 'ä¸é™'} è‡³ {end_date.strftime('%Y-%m-%d') if end_date else 'ä¸é™'})"
    else:
        filtered_articles = all_articles
        date_range = "(å…¨éƒ¨)"

    # å¦‚æœæŒ‡å®šäº†è·å–æœ€æ–°çš„Nç¯‡æ–‡ç« 
    if latest_n is not None:
        filtered_articles = get_latest_n_articles(filtered_articles, latest_n)
        date_range = f"(æœ€æ–° {latest_n} ç¯‡)"

    print(f"ğŸ“ ç¬¦åˆæ¡ä»¶çš„æ–‡ç« æ•°é‡: {len(filtered_articles)} {date_range}")
    
    if len(filtered_articles) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
        return
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å– {len(filtered_articles)} ç¯‡æ–‡ç« ...")
    
    # åœ¨Outputæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºè¾“å‡ºå­æ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    list_name = os.path.splitext(os.path.basename(json_file))[0]
    batch_folder = os.path.join(output_base_dir, f"{list_name}_batch_{timestamp}")
    
    # ç¡®ä¿æ–‡ä»¶å¤¹åç§°å”¯ä¸€
    counter = 1
    original_folder_name = batch_folder
    while os.path.exists(batch_folder):
        batch_folder = f"{original_folder_name}_{counter}"
        counter += 1
    
    # åˆ›å»ºä¸»æ–‡ä»¶å¤¹å’Œå›¾ç‰‡æ–‡ä»¶å¤¹
    os.makedirs(batch_folder, exist_ok=True)
    batch_images_dir = os.path.join(batch_folder, 'images')
    if save_images:
        os.makedirs(batch_images_dir, exist_ok=True)
    
    print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {batch_folder}")
    if save_images:
        print(f"ğŸ“ åˆ›å»ºå›¾ç‰‡æ–‡ä»¶å¤¹: {batch_images_dir}")

    # æå–æ‰€æœ‰é“¾æ¥
    urls = []
    for item in filtered_articles:
        if 'link' in item:
            urls.append(item['link'])
        elif 'url' in item:
            urls.append(item['url'])

    # æ‰¹é‡æŠ“å–æ–‡ç« 
    articles = []
    for idx, url in enumerate(urls, 1):
        print(f"\n[{idx}/{len(urls)}] æ­£åœ¨æŠ“å–: {url}")
        
        try:
            # ç¡®ä¿æ¯ç¯‡æ–‡ç« éƒ½ä½¿ç”¨æ­£ç¡®çš„å›¾ç‰‡ä¿å­˜è·¯å¾„
            article_data = fetch_article_content(url, batch_folder, save_images)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨ä¿å­˜å›¾ç‰‡æ—¶æ˜¾ç¤ºï¼‰
            if save_images:
                print(f"    å›¾ç‰‡ä¿å­˜ç›®å½•: {batch_images_dir}")
                print(f"    æ–‡ç« å›¾ç‰‡æ•°é‡: {len(article_data.get('images', []))}")
            
            articles.append(article_data)
            
            # æ˜¾ç¤ºæŠ“å–ç»“æœ
            if article_data.get('title'):
                print(f"    âœ… æˆåŠŸ: {article_data['title'][:50]}...")
                if save_images and article_data.get('metadata', {}).get('images_saved'):
                    print(f"       ğŸ“¸ å·²ä¿å­˜ {article_data.get('metadata', {}).get('image_count', 0)} å¼ å›¾ç‰‡")
            else:
                print(f"    âŒ å¤±è´¥: æœªè·å–åˆ°æ ‡é¢˜")
            
        except Exception as e:
            print(f"    âŒ æŠ“å–å¼‚å¸¸: {e}")
            articles.append({
                'url': url,
                'title': '',
                'author': '',
                'publish_time': '',
                'read_count': '',
                'like_count': '',
                'content': '',
                'summary': '',
                'error': str(e),
                'content_format': 'plain',
                'images': [],
                'metadata': {
                    'crawl_time': datetime.now().isoformat(),
                    'markdown_enabled': False,
                    'images_saved': False,
                    'image_count': 0,
                    'version': '1.0'
                }
            })
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶å¤¹ï¼ˆä»…åœ¨ä¿å­˜å›¾ç‰‡æ—¶ï¼‰
        if save_images and os.path.exists(batch_images_dir):
            image_files = os.listdir(batch_images_dir)
            print(f"    ğŸ“ å›¾ç‰‡æ–‡ä»¶å¤¹çŠ¶æ€: {len(image_files)} ä¸ªæ–‡ä»¶")
        
        # é˜²æ­¢è¿‡å¿«è¢«å°ï¼Œæ¯æ¬¡æŠ“å–é—´éš” 5 ç§’
        time.sleep(5)

    # ä¿å­˜ç»“æœæ–‡ä»¶
    output_file = os.path.join(batch_folder, "articles_detailed.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for article in articles if article.get('title'))
    fail_count = len(articles) - success_count
    success_rate = success_count/len(articles)*100 if len(articles) > 0 else 0
    
    # åˆ†æå¤±è´¥åŸå› 
    failed_articles = [article for article in articles if not article.get('title')]
    deleted_articles = [article for article in articles if article.get('status') == 'deleted']
    error_analysis = {}
    
    for article in failed_articles:
        error_msg = article.get('error', 'æœªçŸ¥é”™è¯¯')
        error_analysis[error_msg] = error_analysis.get(error_msg, 0) + 1
    
    # ä¿å­˜æŠ“å–ä¿¡æ¯
    info_file = os.path.join(batch_folder, "crawl_info.json")
    crawl_info = {
        "crawl_time": datetime.now().isoformat(),
        "time_range": date_range,
        "start_date": start_date.isoformat() if start_date else None,
        "end_date": end_date.isoformat() if end_date else None,
        "total_articles": len(articles),
        "success_count": success_count,
        "fail_count": fail_count,
        "deleted_count": len(deleted_articles),
        "success_rate": f"{success_rate:.1f}%",
        "source_file": os.path.basename(json_file),
        "error_analysis": error_analysis,
        "format_version": "1.0",
        "markdown_enabled": True,
        "image_support": True,
        "images_dir": "images"
    }
    
    with open(info_file, "w", encoding="utf-8") as f:
        json.dump(crawl_info, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {batch_folder}")
    print(f"   ğŸ“„ æ–‡ç« æ•°æ®: {output_file}")
    print(f"   ğŸ“‹ æŠ“å–ä¿¡æ¯: {info_file}")
    print(f"   ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {os.path.join(batch_folder, 'images')}")

def main():
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡æŠ“å–å·¥å…· (JSON ç‰ˆ)")
    print("ä» ArticleList æ–‡ä»¶å¤¹è¯»å–æ–‡ç« åˆ—è¡¨å¹¶æ‰¹é‡æŠ“å–")
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶å¤¹
    article_list_dir = os.path.join(os.path.dirname(__file__), "ArticleList")
    output_dir = os.path.join(os.path.dirname(__file__), "Output")
    
    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(article_list_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = [f for f in os.listdir(article_list_dir) 
                 if f.endswith('.json') and os.path.isfile(os.path.join(article_list_dir, f))]
    
    if not json_files:
        print(f"\nâŒ åœ¨ ArticleList æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° JSON æ–‡ä»¶")
        print(f"è¯·å°†æ–‡ç« åˆ—è¡¨æ–‡ä»¶æ”¾å…¥: {article_list_dir}")
        return
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶
    print(f"\nğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ç« åˆ—è¡¨æ–‡ä»¶:")
    for i, f in enumerate(json_files, 1):
        print(f"{i}. {f}")
    
    # ç»Ÿä¸€é€‰æ‹©æ—¶é—´èŒƒå›´
    print("\né¦–å…ˆï¼Œè®©æˆ‘ä»¬é€‰æ‹©è¦å¤„ç†çš„æ—¶é—´èŒƒå›´...")
    latest_n = None  # ç”¨äºå­˜å‚¨æœ€æ–°Nç¯‡çš„è®¾ç½®
    
    while True:
        show_time_range_menu()
        choice = input("è¯·é€‰æ‹© (0-13): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")
            return
        
        elif choice == "9":
            # æŠ“å–æ‰€æœ‰æ–‡ç« 
            start_date = None
            end_date = None
            range_name = "æ‰€æœ‰æ–‡ç« "
            break
        
        elif choice == "8":
            # è‡ªå®šä¹‰æ—¶é—´èŒƒå›´
            start_date, end_date = get_custom_date_range()
            if start_date and end_date:
                range_name = f"è‡ªå®šä¹‰èŒƒå›´ ({start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')})"
                break
            else:
                print("âŒ è‡ªå®šä¹‰æ—¶é—´èŒƒå›´è®¾ç½®å¤±è´¥")
                continue
        
        elif choice in ["1", "2", "3", "4", "5", "6", "7"]:
            # é¢„è®¾æ—¶é—´èŒƒå›´
            start_date, end_date, range_name = get_preset_date_range(choice)
            if start_date and end_date:
                break
            else:
                print("âŒ é¢„è®¾æ—¶é—´èŒƒå›´è·å–å¤±è´¥")
                continue
        
        elif choice == "10":
            # æœ€æ–°1ç¯‡ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
            start_date = end_date = None
            latest_n = 1
            range_name = "æœ€æ–°1ç¯‡æ–‡ç«  (å¿«é€Ÿæµ‹è¯•)"
            break
            
        elif choice == "11":
            # æœ€æ–°3ç¯‡ï¼ˆåŸºæœ¬æµ‹è¯•ï¼‰
            start_date = end_date = None
            latest_n = 3
            range_name = "æœ€æ–°3ç¯‡æ–‡ç«  (åŸºæœ¬æµ‹è¯•)"
            break
            
        elif choice == "12":
            # æœ€æ–°10ç¯‡ï¼ˆå®Œæ•´æµ‹è¯•ï¼‰
            start_date = end_date = None
            latest_n = 10
            range_name = "æœ€æ–°10ç¯‡æ–‡ç«  (å®Œæ•´æµ‹è¯•)"
            break
            
        elif choice == "13":
            # è‡ªå®šä¹‰æ•°é‡
            start_date = end_date = None
            latest_n = get_custom_article_count()
            range_name = f"æœ€æ–°{latest_n}ç¯‡æ–‡ç« "
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
    
    # ç»Ÿä¸€é€‰æ‹©æ˜¯å¦çˆ¬å–å›¾ç‰‡
    print("\næ¥ä¸‹æ¥ï¼Œé€‰æ‹©æ˜¯å¦çˆ¬å–æ–‡ç« å›¾ç‰‡...")
    show_crawl_options()
    image_choice = input("è¯·é€‰æ‹© (1-2): ").strip() or "2"
    save_images = image_choice == "1"
    
    if save_images:
        print("\nâš ï¸ æ‚¨é€‰æ‹©äº†åŒæ—¶çˆ¬å–å›¾ç‰‡ï¼Œè¿™å¯èƒ½ä¼šæ˜¾è‘—å¢åŠ çˆ¬å–æ—¶é—´")
    else:
        print("\nâœ“ æ‚¨é€‰æ‹©äº†ä»…çˆ¬å–æ–‡æœ¬ï¼Œè¿™å°†åŠ å¿«çˆ¬å–é€Ÿåº¦")
    
    print(f"\nğŸ“Š å¤„ç†é…ç½®:")
    print(f"æ—¶é—´èŒƒå›´: {range_name}")
    print(f"çˆ¬å–å›¾ç‰‡: {'æ˜¯' if save_images else 'å¦'}")
    print(f"æ–‡ä»¶æ•°é‡: {len(json_files)} ä¸ª")
    if latest_n:
        print(f"æ¯ä¸ªæ–‡ä»¶å¤„ç†: æœ€æ–° {latest_n} ç¯‡æ–‡ç« ")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for i, json_file in enumerate(json_files, 1):
        print(f"\n{'='*50}")
        print(f"å¤„ç†ç¬¬ {i}/{len(json_files)} ä¸ªæ–‡ä»¶: {json_file}")
        print(f"{'='*50}")
        
        full_path = os.path.join(article_list_dir, json_file)
        process_single_list(full_path, output_dir, start_date, end_date, save_images, latest_n)
    
    print("\nâœ¨ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

if __name__ == "__main__":
    main() 