#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹åŠ¨è¾“å…¥å®Œæ•´æ–‡ç« URLå·¥å…·
ç”¨äºå¤„ç†å¾®ä¿¡çŸ­é“¾æ¥æ— æ³•è‡ªåŠ¨å±•å¼€çš„æƒ…å†µ
"""

import json
import os
from datetime import datetime

def load_articles(input_file):
    """åŠ è½½æ–‡ç« æ•°æ®"""
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ç« æ•°æ®å¤±è´¥: {e}")
        return None

def save_articles_with_full_urls(articles, output_file):
    """ä¿å­˜åŒ…å«å®Œæ•´URLçš„æ–‡ç« æ•°æ®"""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def get_full_url_from_user(article):
    """ä»ç”¨æˆ·è·å–å®Œæ•´URL"""
    print(f"\nğŸ“ æ–‡ç« : {article.get('title', 'æ— æ ‡é¢˜')}")
    print(f"   ä½œè€…: {article.get('author', 'æœªçŸ¥')}")
    print(f"   å‘å¸ƒæ—¶é—´: {article.get('publish_time', 'æœªçŸ¥')}")
    print(f"   çŸ­é“¾æ¥: {article.get('url', 'æ— é“¾æ¥')}")
    print()
    
    while True:
        full_url = input("è¯·è¾“å…¥å®Œæ•´æ–‡ç« URL (åŒ…å«__biz, mid, idx, snå‚æ•°): ").strip()
        
        if not full_url:
            print("âŒ URLä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        if not full_url.startswith('https://mp.weixin.qq.com/'):
            print("âŒ è¯·è¾“å…¥æ­£ç¡®çš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URL")
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å‚æ•°
        if '__biz=' in full_url and 'mid=' in full_url and 'idx=' in full_url and 'sn=' in full_url:
            print("âœ… URLæ ¼å¼æ­£ç¡®")
            return full_url
        else:
            print("âŒ URLç¼ºå°‘å¿…è¦å‚æ•°(__biz, mid, idx, sn)ï¼Œè¯·é‡æ–°è¾“å…¥")
            print("   ç¤ºä¾‹: https://mp.weixin.qq.com/s?__biz=MzI4MTI3MzgxNA==&mid=2651234567&idx=1&sn=abcdef123456")

def main():
    print("ğŸ”— æ‰‹åŠ¨è¾“å…¥å®Œæ•´æ–‡ç« URLå·¥å…·")
    print("="*50)
    print()
    print("ç”±äºå¾®ä¿¡çŸ­é“¾æ¥è®¿é—®é™åˆ¶ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥å®Œæ•´URL")
    print("å®Œæ•´URLæ ¼å¼: https://mp.weixin.qq.com/s?__biz=xxx&mid=xxx&idx=xxx&sn=xxx")
    print()
    
    # é€‰æ‹©è¾“å…¥æ–‡ä»¶
    input_files = [
        "articles_detailed_20250720_165023.json",
        "articles_batch_20250720_170202/articles_detailed.json",
        "articles_batch_20250720_170816/articles_detailed.json",
        "articles_batch_20250720_172001/articles_detailed.json"
    ]
    
    print("å¯ç”¨çš„æ–‡ç« æ•°æ®æ–‡ä»¶:")
    for i, file_path in enumerate(input_files, 1):
        if os.path.exists(file_path):
            print(f"   {i}. {file_path}")
    
    print()
    choice = input("è¯·é€‰æ‹©æ–‡ä»¶ç¼–å· (1-4): ").strip()
    
    try:
        file_index = int(choice) - 1
        if 0 <= file_index < len(input_files):
            input_file = input_files[file_index]
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
    except ValueError:
        print("âŒ è¯·è¾“å…¥æ•°å­—")
        return
    
    if not os.path.exists(input_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    # åŠ è½½æ–‡ç« æ•°æ®
    articles = load_articles(input_file)
    if not articles:
        return
    
    print(f"\nğŸ“Š åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« ")
    
    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    updated_articles = []
    skipped_count = 0
    
    for i, article in enumerate(articles, 1):
        print(f"\n[{i}/{len(articles)}] å¤„ç†æ–‡ç« ...")
        
        current_url = article.get('url', '')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å®Œæ•´URL
        if current_url and '__biz=' in current_url and 'mid=' in current_url and 'idx=' in current_url and 'sn=' in current_url:
            print(f"   âœ… å·²ç»æ˜¯å®Œæ•´URLï¼Œè·³è¿‡")
            updated_articles.append(article)
            continue
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦å¤„ç†è¿™ç¯‡æ–‡ç« 
        action = input(f"   æ˜¯å¦å¤„ç†è¿™ç¯‡æ–‡ç« ? (y/n/s=è·³è¿‡æ‰€æœ‰): ").strip().lower()
        
        if action == 's':
            print(f"   â­ï¸  è·³è¿‡å‰©ä½™ {len(articles) - i + 1} ç¯‡æ–‡ç« ")
            # å°†å‰©ä½™æ–‡ç« ç›´æ¥æ·»åŠ 
            updated_articles.extend(articles[i-1:])
            break
        elif action not in ['y', 'yes', 'æ˜¯']:
            print(f"   â­ï¸  è·³è¿‡æ­¤æ–‡ç« ")
            skipped_count += 1
            updated_articles.append(article)
            continue
        
        # è·å–å®Œæ•´URL
        full_url = get_full_url_from_user(article)
        if full_url:
            article['url'] = full_url
            article['full_url_added'] = True
            article['url_updated_at'] = datetime.now().isoformat()
            updated_articles.append(article)
        else:
            print(f"   âŒ è·³è¿‡æ­¤æ–‡ç« ")
            skipped_count += 1
            updated_articles.append(article)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"articles_with_full_urls_{timestamp}.json"
    
    if save_articles_with_full_urls(updated_articles, output_file):
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"   æ€»è®¡: {len(articles)} ç¯‡")
        print(f"   æ›´æ–°: {len([a for a in updated_articles if a.get('full_url_added')])} ç¯‡")
        print(f"   è·³è¿‡: {skipped_count} ç¯‡")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"\nğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰¹é‡è·å–ç»Ÿè®¡æ•°æ®:")
        print(f"   python3 batch_stats_scraper.py --input {output_file} --delay 1.0")

if __name__ == "__main__":
    main() 